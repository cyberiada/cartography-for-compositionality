{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b06877-ac3c-416f-8822-5b25c2b46d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import argparse\n",
    "import scipy.stats\n",
    "import scipy.special as special\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import matplotlib.gridspec as gridspec\n",
    "sns.set(style='whitegrid', font_scale=1.6, context='paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7c76e7-36f3-4ff4-bcd6-5a785f81b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0ded5b-4ced-4360-83e3-71ab5c376772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_path: str) -> Any:\n",
    "\twith open(file_path, \"rb\") as handle:\n",
    "\t\treturn pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847ec7e2-db6b-4647-ae9f-aed719ea5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickle(file: Any, file_path: str) -> None:\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca42a9d-283b-4386-8dcb-066a7c87a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freqs(i2s):\n",
    "    in_v = Counter()\n",
    "    out_v = Counter()\n",
    "    \n",
    "    for txt in i2s[\"In\"]:\n",
    "        tokens = txt.split()\n",
    "        in_v.update(tokens)\n",
    "\n",
    "    for txt in i2s[\"Out\"]:\n",
    "        tokens = txt.split()\n",
    "        out_v.update(tokens)\n",
    "    \n",
    "    total = sum(in_v.values())\n",
    "    for k in in_v:\n",
    "        in_v[k] /= total\n",
    "\n",
    "    total = sum(out_v.values())\n",
    "    for k in out_v:\n",
    "        out_v[k] /= total\n",
    "        \n",
    "    return in_v, out_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29dca1a-bdbb-47ed-9ffb-32083a07f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rarity(in_txt, out_txt, in_v, out_v):\n",
    "    in_toks = in_txt.split()\n",
    "    out_toks = out_txt.split()\n",
    "    \n",
    "    in_rarity, out_rarity = 0, 0\n",
    "    in_len, out_len = len(in_toks), len(out_toks)\n",
    "    \n",
    "    for tok in in_toks:\n",
    "        in_rarity += in_v[tok]\n",
    "        \n",
    "    in_rarity /= in_len\n",
    "    \n",
    "    for tok in out_toks:\n",
    "        out_rarity += out_v[tok]\n",
    "    \n",
    "    out_rarity /= out_len\n",
    "    \n",
    "    return -np.log(in_rarity), -np.log(out_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc03c329-6f3e-40c9-96b5-5f3fda086106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STRING_TRUNCATE = 50\n",
    "\n",
    "def get_scores(dir_path: str, converge_epoch: int, string_truncate: int, min_epoch: int = 3) -> Tuple[Dict[int, Dict[str, List[float]]], Dict[str, List[Any]]]:\n",
    "    file_list = os.listdir(dir_path)\n",
    "    idx_to_sentences: Dict[int, Dict[str, str]] = read_pickle(os.path.join(dir_path, \"idx_to_sentences.pickle\"))\n",
    "\n",
    "    file_list = [f for f in file_list if f[:5] == \"epoch\"]\n",
    "    file_list = [f for f in file_list if int(f.split(\"_\")[0].replace(\"epoch\", \"\")) > min_epoch and int(f.split(\"_\")[0].replace(\"epoch\", \"\")) < converge_epoch]\n",
    "    file_list = sorted(file_list, key= lambda s: int(s.split(\"_\")[1].replace(\"stepidx\", \"\")))\n",
    "\n",
    "    # print(\"Loading files in:\", dir_path)\n",
    "    idxs, ppls, chias, bleus = [], [], [], []\n",
    "    for file_name in file_list:\n",
    "        file_path = f\"{dir_path}/{file_name}\"\n",
    "        #Â print(file_name)\n",
    "        if \"ppl\" in file_path:\n",
    "            ppls.extend(read_pickle(file_path).tolist())\n",
    "        elif \"chia\" in file_path:\n",
    "            chias.extend(read_pickle(file_path).tolist())\n",
    "        elif \"bleu\" in file_path:\n",
    "            bleus.extend(read_pickle(file_path))\n",
    "        elif \"idx\" in file_path:\n",
    "            idxs.extend(read_pickle(file_path).tolist())\n",
    "        else:\n",
    "            output_csv_name = file_path\n",
    "\n",
    "    items = list(zip(idxs, ppls, chias, bleus))\n",
    "    items = sorted(items, key=lambda i: i[0])\n",
    "    idx_dict: Dict[int, Dict[str, List[float]]] = {}\n",
    "    for item in items:\n",
    "        if item[0] not in idx_dict:\n",
    "            idx_dict[item[0]] = {\"inv_ppl\": [1 / item[1]], \"chia\": [item[2]], \"bleu\": [item[3]]}\n",
    "        else:\n",
    "            idx_dict[item[0]][\"inv_ppl\"].append(1 / item[1])\n",
    "            idx_dict[item[0]][\"chia\"].append(item[2])\n",
    "            idx_dict[item[0]][\"bleu\"].append(item[3])\n",
    "\n",
    "    i2s = {\"Index\": [], \"In\": [], \"Out\": [], \"In abbv.\": [], \"Out abbv.\": [], \"In Len\": [], \"Out Len\": [], \"In Rarity\": [], \"Out Rarity\": []}\n",
    "\n",
    "    for k, v in idx_to_sentences.items():\n",
    "        i2s[\"Index\"].append(k)\n",
    "        i2s[\"In\"].append(v[\"in\"])\n",
    "        i2s[\"Out\"].append(v[\"out\"])\n",
    "        i2s[\"In abbv.\"].append(v[\"in\"][:STRING_TRUNCATE])\n",
    "        i2s[\"Out abbv.\"].append(v[\"out\"][:STRING_TRUNCATE])\n",
    "        i2s[\"In Len\"].append(len(v[\"in\"].split()))\n",
    "        i2s[\"Out Len\"].append(len(v[\"out\"].split()))\n",
    "\n",
    "    in_v, out_v = get_word_freqs(i2s)\n",
    "    for k, v in idx_to_sentences.items():\n",
    "        in_rarity, out_rarity = get_rarity(v[\"in\"], v[\"out\"], in_v, out_v)\n",
    "        i2s[\"In Rarity\"].append(in_rarity)\n",
    "        i2s[\"Out Rarity\"].append(out_rarity)\n",
    "\n",
    "    return idx_dict, i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fff27f-57cc-4a75-b76e-0efe37123d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocab(df):\n",
    "\tin_v = Counter()\n",
    "\tout_v = Counter()\n",
    "    \n",
    "\tfor idx, txt in df[\"In\"].items():\n",
    "\t\ttokens = txt.split()\n",
    "\t\tin_v.update(tokens)\n",
    "         \n",
    "\tfor idx, txt in df[\"Out\"].items():\n",
    "\t\ttokens = txt.split()\n",
    "\t\tout_v.update(tokens)\n",
    "\n",
    "\treturn set(in_v.keys()), set(out_v.keys()), in_v, out_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde8fd88-8e0c-4848-83c8-440704b82bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(epoch: int, idx_dict: Dict[int, Dict[str, List[float]]], i2s: Dict[str, List[Any]]) -> pd.DataFrame:\n",
    "    idx_mean_var_dict: Dict[int, Dict[str, Tuple[float, float]]] = {}\n",
    "    idx_mean_var_list: List[Tuple[int, float, float, float, float, float, float, float, float]] = []\n",
    "    \n",
    "    bins = np.linspace(0.0, 1.0, 10)\n",
    "    score_names = [\"inv_ppl\", \"chia\", \"bleu\"]\n",
    "    for idx, scores in idx_dict.items():\n",
    "        scores_list = []\n",
    "        for score_name in score_names:\n",
    "            score_arr = np.array(scores[score_name][:epoch])\n",
    "            mean = score_arr.mean()\n",
    "            var = score_arr.var()\n",
    "            scores_list.extend([mean, var])\n",
    "            if score_name == \"bleu\":\n",
    "                correctness = np.isclose(score_arr, 1.0).mean()\n",
    "                correctness = np.digitize(correctness, bins, right=False) * 0.1\n",
    "                scores_list.append(correctness)\n",
    "        \n",
    "        \n",
    "        idx_mean_var_list.append(tuple((idx, *scores_list)))\n",
    "\n",
    "    i2s_df = pd.DataFrame.from_dict(i2s)\n",
    "\n",
    "    df = pd.DataFrame(idx_mean_var_list, columns =['Index', 'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "                                                    'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "                                                    'Confidence - BLEU', 'Variability - BLEU', 'Correctness'])\n",
    "\n",
    "    cartography = pd.merge(df, i2s_df, on=\"Index\")\n",
    "    \n",
    "    df[\"Correctness\"] = df[\"Correctness\"].apply(lambda x: round(x, 1))\n",
    "\n",
    "    return cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e43912-8561-4067-9c89-b271bbe079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(dir_path: str, plot_path: str, converge_epoch: int) -> None:\n",
    "\tidx_dict = get_scores(dir_path, plot_path, converge_epoch)\n",
    "\t\n",
    "\tfor epoch in trange(3, converge_epoch, 2):\n",
    "\t\tdf = calculate_statistics(epoch, idx_dict)\n",
    "\n",
    "\t\tplot_types = [\"inv_ppl\", \"chia\", \"bleu\"]\n",
    "\n",
    "\t\tfor plot_type in tqdm(plot_types, \"Plots\"):\n",
    "\t\t\tplot(df, plot_path, str(epoch), plot_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29c2fb0-7401-4cf7-b57c-8451ece5082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subset(subset_df: pd.DataFrame, ds_name: str, subset_fname: str) -> None:\n",
    "    subset_idx = subset_df[\"Index\"].tolist()\n",
    "    subset_idx = [int(i) for i in subset_idx]\n",
    "    subset_idx = set(subset_idx)\n",
    "    \n",
    "    os.makedirs(os.path.join(\"subsets\", ds_name), exist_ok=True)\n",
    "    write_pickle(subset_idx, os.path.join(\"subsets\", ds_name, subset_fname))\n",
    "    print(f\"subset_idx: {len(subset_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f2ab5c-4900-4104-884e-dc926d1141e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def choose_subset(df: pd.DataFrame, metric: str, criteria: str, ds_name: str, subset_fname:str, ratio:float = 0.33, write=True) -> pd.DataFrame:\n",
    "    assert metric in [\"Inverse PPL\", \"Neg PPL\", \"CHIA\", \"BLEU\"]\n",
    "    assert criteria in [\"Easy to Learn\", \"Ambiguous\", \"Hard to Learn\", \"Random\"]\n",
    "    \n",
    "    if criteria == \"Easy to Learn\":\n",
    "        sort_by = f\"Confidence - {metric}\"\n",
    "        ascending = False\n",
    "    elif criteria == \"Ambiguous\":\n",
    "        sort_by = f\"Variability - {metric}\"\n",
    "        ascending = False\n",
    "    elif criteria == \"Hard to Learn\":\n",
    "        sort_by = f\"Confidence - {metric}\"\n",
    "        ascending = True\n",
    "        \n",
    "    if criteria == \"Random\":\n",
    "        sorted_df = df.sample(frac=1)\n",
    "    else:\n",
    "        sorted_df = df.sort_values(by=[sort_by], ascending=ascending)\n",
    "\n",
    "    sorted_df = sorted_df.reset_index(drop=True)\n",
    "    subset_df = sorted_df.iloc[:int(len(df)*ratio),:]\n",
    "    \n",
    "    all_in_v, all_out_v, _, _ = create_vocab(df)\n",
    "    subset_in_v, subset_out_v, subset_in_v_counts, subset_out_v_counts = create_vocab(subset_df)\n",
    "\n",
    "    add_ex_i = []\n",
    "    remove_ex_i = []\n",
    "    \n",
    "    for i in trange(int(len(df)*ratio), len(df)):\n",
    "        new_in, new_out = sorted_df.iloc[i, 7], sorted_df.iloc[i, 8]\n",
    "        new_in_tokens, new_out_tokens = set(new_in.split()), set(new_out.split())\n",
    "        \n",
    "        if (new_in_tokens - subset_in_v) or (new_out_tokens - subset_out_v):\n",
    "            add_ex_i.append(i)\n",
    "            subset_in_v = subset_in_v.union(new_in_tokens)\n",
    "            subset_out_v = subset_out_v.union(new_out_tokens)\n",
    "            subset_in_v_counts.update(new_in.split())\n",
    "            subset_out_v_counts.update(new_out.split())\n",
    "            \n",
    "    in_counter = subset_in_v_counts\n",
    "    out_counter = subset_out_v_counts\n",
    "    \n",
    "    removed_amount = 0\n",
    "    for i in trange(int(len(df)*ratio)-1, -1, -1):\n",
    "        if len(remove_ex_i) == len(add_ex_i):\n",
    "            break\n",
    "        \n",
    "        print(sorted_df.iloc[i, 7], sorted_df.iloc[i, 8], sorted_df.iloc[i, 9])\n",
    "        ex_in, ex_out = sorted_df.iloc[i, 7], sorted_df.iloc[i, 8]\n",
    "        ex_in_counter, ex_out_counter = Counter(ex_in.split()), Counter(ex_out.split())\n",
    "        \n",
    "        upd_in_counter = in_counter - ex_in_counter\n",
    "        upd_out_counter = out_counter - ex_out_counter\n",
    "        \n",
    "        ex_in_words, ex_out_words = list(set(ex_in.split())), list(set(ex_out.split()))\n",
    "        \n",
    "        remove = True\n",
    "        for word in ex_in_words:\n",
    "            if upd_in_counter[word] <= 1:\n",
    "                remove = False\n",
    "        \n",
    "        for word in ex_out_words:\n",
    "            if upd_out_counter[word] <= 1:\n",
    "                remove = False\n",
    "                \n",
    "        if remove:\n",
    "            in_counter = upd_in_counter\n",
    "            out_counter = upd_out_counter\n",
    "            remove_ex_i.append(i)\n",
    "    \n",
    "    subset_df = pd.concat([subset_df, df.iloc[add_ex_i]])\n",
    "    subset_df = subset_df.drop(remove_ex_i, axis=0)\n",
    "    subset_df = subset_df.reset_index(drop=True)\n",
    "    \n",
    "    assert all_in_v == set(in_counter.keys()), \"The process is wrong\"\n",
    "    assert all_out_v == set(out_counter.keys()), \"The process is wrong 2\"\n",
    "    \n",
    "    if write:\n",
    "        save_subset(subset_df, ds_name, subset_fname)\n",
    "    \n",
    "    print(len(remove_ex_i), len(add_ex_i))\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b5126d-f2fa-4904-9585-ff9e03ecc01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsets(df: pd.DataFrame, subset_dfs: List[pd.DataFrame], ds_name: str, subset_fname: str) -> pd.DataFrame:\n",
    "    \n",
    "    combined_set = pd.concat(subset_dfs)\n",
    "    combined_set = combined_set.drop_duplicates(keep=\"first\")\n",
    "    \n",
    "    if len(combined_set) > (len(df) / 2):\n",
    "        combined_set = combined_set.iloc[:int(len(df) / 2)]\n",
    "    else:\n",
    "        count = 0\n",
    "        while len(combined_set) < (len(df) / 2):\n",
    "            example = df.sample(n=1)\n",
    "            if not example.iloc[0][\"In\"] in combined_set['In'].tolist():\n",
    "                combined_set = combined_set.append(example)\n",
    "                \n",
    "    save_subset(combined_set, ds_name, subset_fname)\n",
    "    \n",
    "    return combined_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2075ae2-2739-4421-97be-73ff56a4ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, plot_type, dataset, converge_epoch, double_width):\n",
    "    width_scale = 2 if double_width else 1\n",
    "    sns.set(style='whitegrid', font_scale=1.6 * width_scale, context='paper')\n",
    "    show_hist: bool = True\n",
    "    hue_metric: str = 'Correctness'\n",
    "    main_metric = f'Variability - {abv2mtrc[plot_type]}'\n",
    "    other_metric = f'Confidence - {abv2mtrc[plot_type]}'\n",
    "\n",
    "    hue = 'Correctness'\n",
    "    num_hues = len(df[hue].unique().tolist())\n",
    "    style = hue_metric\n",
    "\n",
    "    if not show_hist:\n",
    "        fig, ax0 = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(14, 12), )\n",
    "        gs = fig.add_gridspec(3, 2, width_ratios=[5, 1])\n",
    "        ax0 = fig.add_subplot(gs[:, 0])\n",
    "\n",
    "    pal = sns.diverging_palette(260, 15, n=num_hues, sep=10, center=\"dark\")\n",
    "\n",
    "    plot = sns.scatterplot(x=main_metric, y=other_metric, ax=ax0, data=df, hue=hue, palette=pal, style=style, s=30)\n",
    "    if not show_hist:\n",
    "        plot.legend(ncol=1, bbox_to_anchor=[0.175, 0.5], loc='right')\n",
    "    else:\n",
    "        plot.legend(fancybox=True, shadow=True,  ncol=1)\n",
    "    \n",
    "    for text in plot.get_legend().texts:\n",
    "        text._text = str(round(float(text._text), 1))\n",
    "        \n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    plot.set_xlabel(main_metric)\n",
    "    plot.set_ylabel(other_metric)\n",
    "\n",
    "    if show_hist:\n",
    "        plot.set_title(f\"{abv2mtrc[plot_type]} Data Map\", fontsize=17)\n",
    "\n",
    "        # Make the histograms.\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(gs[1, 1])\n",
    "        ax3 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "        plott0 = df.hist(column=[other_metric], ax=ax1, color='#622a87')\n",
    "        plott0[0].set_title('')\n",
    "        plott0[0].set_xlabel('Confidence')\n",
    "        plott0[0].set_ylabel('Density')\n",
    "\n",
    "        plott1 = df.hist(column=[main_metric], ax=ax2, color='teal')\n",
    "        plott1[0].set_title('')\n",
    "        plott1[0].set_xlabel('Variability')\n",
    "        plott1[0].set_ylabel('Density')\n",
    "\n",
    "        plot2 = sns.countplot(x=\"Correctness\", data=df, ax=ax3, color='#86bf91')\n",
    "        labels = [str(round(float(text._text), 1)) for text in ax3.get_xticklabels()]\n",
    "        ax3.set_xticklabels(labels, rotation=50, ha='right')\n",
    "        \n",
    "        ax3.xaxis.grid(True) # Show the vertical gridlines\n",
    "\n",
    "        plot2.set_title('')\n",
    "        plot2.set_xlabel('Correctness')\n",
    "        plot2.set_ylabel('Density')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    os.makedirs(f'plots/{dataset}/{plot_type}', exist_ok=True)\n",
    "    filename = f'plots/{dataset}/{plot_type}/{converge_epoch}_{\"double\" if double_width else \"single\"}.pdf'\n",
    "    fig.savefig(filename, format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0386cacd-8d81-4d94-aa31-dcf8606f950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_TRUNCATE = 120\n",
    "\n",
    "mtrc2abv = {\"Inverse PPL\": \"inv_ppl\", \"Neg PPL\": \"neg_ppl\", \"CHIA\": \"chia\", \"BLEU\": \"bleu\"}\n",
    "abv2mtrc = {v:k for k, v in mtrc2abv.items()}\n",
    "crit2abv = {\"Easy to Learn\": \"easy_to_learn\", \"Ambiguous\": \"ambiguous\", \"Hard to Learn\": \"hard_to_learn\", \"Random\": \"random\"}\n",
    "create_fname = lambda m, cr, c_e: f\"{mtrc2abv[m]}_{crit2abv[cr]}_{c_e}.pickle\"\n",
    "create_ratio_fname = lambda m, cr, c_e, rto: f\"{mtrc2abv[m]}_{crit2abv[cr]}_{c_e}_{rto}.pickle\"\n",
    "create_comb_fname = lambda m, cr1, cr2, c_e: f\"{mtrc2abv[m]}_{crit2abv[cr1]}_{crit2abv[cr2]}_{c_e}.pickle\"\n",
    "outputs_path = lambda x: f\"../scores/{x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ded4f-6ad1-45c3-823e-0609df0ac4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGE_EPOCHS = {\n",
    "    \"0/cfq\": 20,\n",
    "    \"0/cogs\": 10,\n",
    "    \"42/cfq\": 20,\n",
    "    \"42/cogs\": 10,\n",
    "    \"123/cfq\": 20,\n",
    "    \"123/cogs\": 10\n",
    "}\n",
    "\n",
    "ALL_METRICS = [\"inv_ppl\", \"chia\", \"bleu\"],\n",
    "\n",
    "METRICS = {\n",
    "    \"0/cfq\": ALL_METRICS,\n",
    "    \"0/cogs\": ALL_METRICS,\n",
    "    \"42/cfq\": ALL_METRICS,\n",
    "    \"42/cogs\": ALL_METRICS,\n",
    "    \"123/cfq\": ALL_METRICS,\n",
    "    \"123/cogs\": ALL_METRICS\n",
    "}\n",
    "\n",
    "for DATASET_NAME in CONVERGE_EPOCHS.keys():\n",
    "    OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "    CONVERGE_EPOCH = CONVERGE_EPOCHS[DATASET_NAME]\n",
    "    idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "    df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "    for METRIC in METRICS[DATASET_NAME]::\n",
    "            print(DATASET_NAME, METRIC, DOUBLE_WIDTH)\n",
    "            plot(df, METRIC, DATASET_NAME, CONVERGE_EPOCH, double_width=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
